{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First little Network\n",
    "This class explores a first little neural network structure for the CT data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import dicom\n",
    "import os\n",
    "from matplotlib import pyplot as plt, cm\n",
    "import time\n",
    "from matplotlib import animation, rc\n",
    "from cvloop import cvloop\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_patient(path):\n",
    "    \"\"\"\n",
    "    Returns a sorted List of scans that are found in one specific folder.\n",
    "    \"\"\"\n",
    "    all_scans = []  # create an empty list\n",
    "    for dirName, subdirList, fileList in os.walk(path):\n",
    "        for filename in fileList:\n",
    "            if \".dcm\" in filename.lower():  # check whether the file's DICOM\n",
    "                all_scans.append(dicom.read_file(os.path.join(dirName,filename)))\n",
    "    all_scans.sort(key = lambda x: int(x.InstanceNumber))\n",
    "    # TODO: normalize images!!\n",
    "    return all_scans\n",
    "\n",
    "def conv2array(scan_files):\n",
    "    \"\"\"\n",
    "    Converts the dicom files of one patient to a numpy array. \n",
    "    \"\"\"\n",
    "    ref_scan = scan_files[0]\n",
    "    # Load dimensions based on the number of rows, columns, and slices (along the Z axis)\n",
    "    pixel_dims = (int(ref_scan.Rows), int(ref_scan.Columns), len(scan_files))\n",
    "\n",
    "    # Load spacing values (in mm)\n",
    "    pixel_spacing = (float(ref_scan.PixelSpacing[0]), float(ref_scan.PixelSpacing[1]), float(ref_scan.SliceThickness))\n",
    "\n",
    "    x = np.arange(0.0, (pixel_dims[0]+1)*pixel_spacing[0], pixel_spacing[0])\n",
    "    y = np.arange(0.0, (pixel_dims[1]+1)*pixel_spacing[1], pixel_spacing[1])\n",
    "    z = np.arange(0.0, (pixel_dims[2]+1)*pixel_spacing[2], pixel_spacing[2])\n",
    "\n",
    "    # The array is sized based on 'pixel_dims'\n",
    "    array_imgs = np.zeros(pixel_dims, dtype=ref_scan.pixel_array.dtype)\n",
    "\n",
    "    # loop through all the DICOM files\n",
    "    for i in range(len(scan_files)):\n",
    "        # store the raw image data\n",
    "        array_imgs[:, :, i] = scan_files[i].pixel_array\n",
    "    return array_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scan_pat = read_patient(\"../../data/LIDC-IDRI-0666/1.3.6.1.4.1.14519.5.2.1.6279.6001.150264634200093580367988090366/1.3.6.1.4.1.14519.5.2.1.6279.6001.325580698241281352835338693869/\")\n",
    "array_patient = conv2array(scan_pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class data:\n",
    "    def __init__(self, scans):\n",
    "        self.scans = scans\n",
    "        self.i = 0\n",
    "            \n",
    "    def read(self):\n",
    "        self.i = self.i + 1\n",
    "        return True, self.scans[:,:,self.i]\n",
    "    \n",
    "cvloop(data(array_patient), function=lambda x: 1-x, side_by_side=True, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow Graph Definition\n",
    "\n",
    "def get_input(patient_scan, qube_size=(5,5,3)):\n",
    "    \"\"\"\n",
    "    Get a random cube out of a random scan (?) and have the network put out 0 or 1 for nodule or not?\n",
    "    But that would be underrepresenting the nodule containing segments.. So first compute a database of nodules\n",
    "    and a database of non-nodule patches? And in the end have a algorithm that goes over the \n",
    "    whole scan and outputs 1, 0 for the whole thing?! \n",
    "    \"\"\"\n",
    "    return patient_scan\n",
    "    \n",
    "    \n",
    "\n",
    "def new_conv_layer(input, num_input_channels, filter_size, num_filters, use_pooling=True):\n",
    "    shape = [filter_size, filter_size,  num_input_channels, num_filters]\n",
    " \n",
    "    weights = new_weights(shape=shape)\n",
    " \n",
    "    biases = new_biases(length=num_filters)\n",
    " \n",
    "    layer = tf.nn.conv3d(input=input, \n",
    "                         filter=weights, \n",
    "                         strides=[1, 1, 1, 1], \n",
    "                         padding='SAME')\n",
    "    layer += biases\n",
    " \n",
    "    if use_pooling:\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer, weights\n",
    "\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    " \n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    return layer_flat, num_features\n",
    "\n",
    "\n",
    "def new_fc_layer(input,      \n",
    "             num_inputs,     \n",
    "             num_outputs,    \n",
    "             use_relu=True): \n",
    "\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size_flat= img_size * img_size * num_channels\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    " \n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array_patient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# think about (train 9/10, validation 1/10) 2/3 and test 1/3\n",
    "# train until performance get's worse on validation set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
