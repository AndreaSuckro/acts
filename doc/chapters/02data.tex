\graphicspath{ {imgs/} }
\documentclass[../Thesis.tex]{subfiles}
\begin{document}
\chapter{Dataset}
The Lung Image Database Consortium image collection (LIDC-IDRI) consists of diagnostic and lung cancer screening thoracic computed tomography (CT) scans with marked-up annotated lesions. It is a web-accessible international resource for development, training, and evaluation of computer-assisted diagnostic (CAD) methods for lung cancer detection and diagnosis. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process.
Seven academic centers and eight medical imaging companies collaborated to create this data set which contains 1018 cases.  Each subject includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories ("nodule > or =3 mm," "nodule <3 mm," and "non-nodule > or =3 mm"). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus. \cite{armato2011lung}

\section{Content and Structure}
The dataset contains a folder for each patient. These folders contain a full chest CT scan and the annotations done by the radiologists. The CT scan is encoded in a list of .dcm files in DICOM formatting and the annotations as one .xml file. The structure of both file types is described in the upcoming sections.

\subsection{Scan Data Structure}
DICOM (abbreviation for Digital Imaging and COmmunications in Medicine) is a file format for storing medical images with for the use case relevant meta information. It is not only used for CT scans but also for radiography, ultrasonography and MRI data. It was initially introduced by the American College of Radiology (ACR) and National Electrical Manufacturers Association (NEMA) under the name ACR/NEMA 300 in 1985, but further redefined and finally in the third version released under the name DICOM in 1993\cite{pianykh2008}.

The cases that scans are provided for in the dataset fullfill certain criteria. The CT scans are exclusively of the lung only and do not contain other body parts or organs. The  reconstruction interval and collimation are kept at $<3mm$. This means that there are differences in the resolution of the scan data (naturally through the different equipment used during recording), but that it is limited with an upper bound. Still one can find patients with 124-529 recorded images in the data. The scan data may also include noise or other disruptive factors like metal (heart pacers etc.) as well as other pathological features as long as those do not interfere with the visibility of the nodules in a drastic sense.

The included cases have between 0 and 6 nodules with a longest diameter between 3-30mm. The term nodule refers to a brought spectrum of tissue abnormalities and can represent not only lung cancer but also other metastatic diseases or non-cancerous processes or lesions that have a nodular morphology. Typical slices from the data can be seen in 

\begin{figure}[!tbp]
\centering
\begin{minipage}[b]{0.45\textwidth}
	\includegraphics[width=\textwidth]{slice1.png}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
	\includegraphics[width=\textwidth]{slice2.png}
\end{minipage}
\caption{Two slices from different patients in the dataset. Images have been generated with Osirix Lite \cite{rosset2004osirix}}
\label{fig:slices}
\end{figure}

\subsection{Annotation Structure}
Two different types of nodules are encoded in the data: nodules with a diameter of $>=$ 3mm and nodules smaller than that. The big nodules have extensive information stored with them: a rich edge map which outlines a complete contour for them in all sections \ref{fig:bigNod} and a measure for their characteristics (like their subtlety and malignancy on a scale from 1 to 5). Those extra information have not been used in the learning process for this thesis.

\lstset{
  language=XML,
  morekeywords={characteristics,noduleID,edgeMap,imageZposition}
}
\begin{figure}
\begin{lstlisting}
      <noduleID>IL057_127581</noduleID>
      <characteristics>
        <subtlety>4</subtlety>
        <malignancy>3</malignancy>
        [...]
      </characteristics>
      
      <edgeMap>
        <xCoord>103</xCoord>
        <yCoord>391</yCoord>
      </edgeMap>
 
      <imageZposition>-232.535004</imageZposition>
       
      <edgeMap>
         <xCoord>104</xCoord>
         <yCoord>393</yCoord>
      </edgeMap>
\end{lstlisting}
\caption{A shortened example xml annotation for a nodule with diameter $>=$ 3mm.}
\label{fig:bigNod}
\end{figure}

Nodules with a smaller diameter have less information stored with them. They only contain the approximate center of mass for the nodule \ref{fig:smallNod}.

\begin{figure}
\begin{lstlisting}
	<noduleID>7</noduleID>
	<roi>
	<imageZposition>-227.535004</imageZposition>
        <imageSOP_UID>1.3.6.1.4...</imageSOP_UID>
	<inclusion>TRUE</inclusion>
	<edgeMap>
	   <xCoord>127</xCoord>
	   <yCoord>370</yCoord>
	</edgeMap>
	</roi>
\end{lstlisting}
\caption{Nodules with a diameter of $<$ 3mm have only the center of mass stored.}
\label{fig:smallNod}
\end{figure}

\section{Properties of the Data}

The two classes the network should differentiate were healthy patches and patches that contain a tumor. 1000 samples from the two classes 

\section{Preprocessing}
The data is available in the form of sub-folders for each patient that contain the CT scan results in .dicom file format and the annotation data as xml files. The following sections describe the read in and preprocessing of the data.

\subsection{Reading in the data}
Initially the whole dataset was manually randomly distributed to 3 folders in a $50:25:25$ split ratio: train, validation and test. The training dataset is used during the learning process of the neural network. The testing dataset is used to measure the performance of the network while it is trained on the train dataset. See \ref{fig:ttdist} for a distribution of values for the two classes. 
\begin{figure}
\includegraphics[width=\textwidth]{distribution_class.png}
\caption{Distribution of the normalized pixel values for both classes in the training and test set (1000 instances per set)}
\label{fig:ttdist}
\end{figure}
Each of the folders contains a list of patients containing one ore more sub-folders with CT scans. Some of the extra folders for a patient contain only a few scans and not a complete CT. Those folders were ignored. With the use of the Python package dicom \cite{mason2011t} the ct scans are converted to a 3 dimensional numpy array. The annotations are evaluated to find the location the nodules. In the case of nodules with a diameter of $<3mm$ the center of mass value is used, for the bigger nodules that have information of the whole edgemap, the mean over all dimensions is used as an crude approximation for the center of mass of those nodules. This information is used to generate the patches from the complete scan. A fixed number of patches is generated from the data per patient. The patches for the healthy data are cut randomly from the tissue that contain no nodules while the patches with the nodule information are centered around the nodule's center of mass.

\end{document}
